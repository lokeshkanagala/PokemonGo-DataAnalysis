=========  Instructions to Run the code in AWS Elastic Map Reduce=================1. Create a EMR cluster with Apache Spark Configuration. 2. Create a S3 bucket in Amazon web services, and load your Dataset and source code files to the bucket.3. Connect to the cluster via your local Terminal using SSH and type the following commands:4. aws s3  cp s3://<bucketname>/<sourcecodefile>  ./aws s3 cp s3://<bucketname>/<Dataset>  ./spark-submit <sourcecode.py> s3n://<bucketname>/<inputDataset>  <Timezone> <Weekday>=========  Instructions to Run the code in the UNCC Hadoop cluster =================open the terminal and connect to DSBA cluster using the following ssh <username>@dsba-hadoop.uncc.eduCreate a folder to place all the files under /users/<username> mkdir /users/<username>/<folderName>open another terminal to copy files from local system to cluster by using the following scp commandscp <filename> <local path where file is kept> <username>@dsba-hadoop.uncc.edu:<path of the cluster>now open the terminal from which you have connected to the clustercopy the input files from cluster to hdfs using the following commandhadoop fs -put <filepath in the cluster>/filename <input path in hdfs>submit the job using following commandspark-submit <.py filename> <input file along with the path in hdfs> <Timezone> <Weekday>